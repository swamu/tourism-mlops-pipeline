{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klg2JF-oBblG"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0CcOjZ-BblL"
   },
   "source": [
    "## **Business Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyT6Koe7BblM"
   },
   "source": [
    "\"Visit with Us,\" a leading travel company, is revolutionizing the tourism industry by leveraging data-driven strategies to optimize operations and customer engagement. While introducing a new package offering, such as the Wellness Tourism Package, the company faces challenges in targeting the right customers efficiently. The manual approach to identifying potential customers is inconsistent, time-consuming, and prone to errors, leading to missed opportunities and suboptimal campaign performance.\n",
    "\n",
    "To address these issues, the company aims to implement a scalable and automated system that integrates customer data, predicts potential buyers, and enhances decision-making for marketing strategies. By utilizing an MLOps pipeline, the company seeks to achieve seamless integration of data preprocessing, model development, deployment, and CI/CD practices for continuous improvement. This system will ensure efficient targeting of customers, timely updates to the predictive model, and adaptation to evolving customer behaviors, ultimately driving growth and customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm6bNQOJBblO"
   },
   "source": [
    "## **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PYtjk_YBblO"
   },
   "source": [
    "This project involves working as an MLOps Engineer at \"Visit with Us\" to design and deploy a complete MLOps pipeline on GitHub. The main challenge is automating the entire workflow for predicting which customers are likely to purchase the newly introduced Wellness Tourism Package.\n",
    "\n",
    "**What This Project Delivers:**\n",
    "- A predictive model to identify potential customers before contacting them\n",
    "- An automated pipeline covering data cleaning, preprocessing, model training, and deployment\n",
    "- A CI/CD system using GitHub Actions for seamless updates and deployment\n",
    "- Integration with Hugging Face for model hosting and Streamlit for the web interface\n",
    "\n",
    "**Project Impact:**\n",
    "This solution helps the marketing team make data-driven decisions, improve campaign targeting, and ultimately increase customer acquisition. By automating the entire process, the model stays up-to-date and performs consistently as customer behavior evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "Starting with importing all the necessary libraries for this MLOps pipeline. The imports are organized into logical groups (data processing, machine learning, MLflow tracking, etc.) to keep the code clean and maintainable. Make sure all packages are installed before running the subsequent cells!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "\n",
    "Before diving into the code, upgrading all key packages to their latest versions helps avoid compatibility issues down the line. This cell installs or upgrades `huggingface_hub`, `pandas`, `scikit-learn`, `xgboost`, `mlflow`, and `datasets`.\n",
    "\n",
    "**Important:** After running this cell, restart the kernel to ensure all updates take effect properly!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrading key libraries to ensure compatibility across the pipeline\n",
    "# This prevents version conflicts that could cause import errors later\n",
    "%pip install --upgrade huggingface_hub pandas scikit-learn xgboost mlflow datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swmukherjee/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face - with error handling for version compatibility\n",
    "try:\n",
    "    from huggingface_hub import HfApi, create_repo, hf_hub_download\n",
    "except ImportError as e:\n",
    "    print(\"⚠ Hugging Face Hub import failed. Run: %pip install --upgrade huggingface_hub and restart kernel\")\n",
    "    # Define placeholder to prevent errors in subsequent cells\n",
    "    HfApi = None\n",
    "    create_repo = None\n",
    "    hf_hub_download = None\n",
    "\n",
    "# Scikit-learn - Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scikit-learn - Model Selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Scikit-learn - Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Scikit-learn - Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8C11AzTBblP"
   },
   "source": [
    "## **Data Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DQx3pkaBblP"
   },
   "source": [
    "The dataset contains customer and interaction data that serve as key attributes for predicting the likelihood of purchasing the Wellness Tourism Package. The detailed attributes are:\n",
    "\n",
    "**Customer Details**\n",
    "- **CustomerID:** Unique identifier for each customer.\n",
    "- **ProdTaken:** Target variable indicating whether the customer has purchased a package (0: No, 1: Yes).\n",
    "- **Age:** Age of the customer.\n",
    "- **TypeofContact:** The method by which the customer was contacted (Company Invited or Self Inquiry).\n",
    "- **CityTier:** The city category based on development, population, and living standards (Tier 1 > Tier 2 > Tier 3).\n",
    "- **Occupation:** Customer's occupation (e.g., Salaried, Freelancer).\n",
    "- **Gender:** Gender of the customer (Male, Female).\n",
    "- **NumberOfPersonVisiting:** Total number of people accompanying the customer on the trip.\n",
    "- **PreferredPropertyStar:** Preferred hotel rating by the customer.\n",
    "- **MaritalStatus:** Marital status of the customer (Single, Married, Divorced).\n",
    "- **NumberOfTrips:** Average number of trips the customer takes annually.\n",
    "- **Passport:** Whether the customer holds a valid passport (0: No, 1: Yes).\n",
    "- **OwnCar:** Whether the customer owns a car (0: No, 1: Yes).\n",
    "- **NumberOfChildrenVisiting:** Number of children below age 5 accompanying the customer.\n",
    "- **Designation:** Customer's designation in their current organization.\n",
    "- **MonthlyIncome:** Gross monthly income of the customer.\n",
    "\n",
    "**Customer Interaction Data**\n",
    "- **PitchSatisfactionScore:** Score indicating the customer's satisfaction with the sales pitch.\n",
    "- **ProductPitched:** The type of product pitched to the customer.\n",
    "- **NumberOfFollowups:** Total number of follow-ups by the salesperson after the sales pitch.-\n",
    "- **DurationOfPitch:** Duration of the sales pitch delivered to the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DtS3gNDjBbR"
   },
   "source": [
    "# Stage 1: Data Registration\n",
    "\n",
    "## Overview\n",
    "This section focuses on registering the tourism dataset to Hugging Face Hub for centralized data management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Upload Script\n",
    "\n",
    "To register the tourism dataset on Hugging Face Hub for centralized data management, version control, and easy accessibility across the MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face token from environment\n",
    "# Make sure to set HF_TOKEN in your environment:\n",
    "# - Run: source setup_env.sh (or set it manually in your shell)\n",
    "# - Or create a .env file with: HF_TOKEN=your_token_here\n",
    "hf_token = os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4128, 21)\n",
      "   Unnamed: 0  CustomerID  ProdTaken   Age    TypeofContact  CityTier  \\\n",
      "0           0      200000          1  41.0     Self Enquiry         3   \n",
      "1           1      200001          0  49.0  Company Invited         1   \n",
      "2           2      200002          1  37.0     Self Enquiry         1   \n",
      "3           3      200003          0  33.0  Company Invited         1   \n",
      "4           5      200005          0  32.0  Company Invited         1   \n",
      "\n",
      "   DurationOfPitch   Occupation  Gender  NumberOfPersonVisiting  ...  \\\n",
      "0              6.0     Salaried  Female                       3  ...   \n",
      "1             14.0     Salaried    Male                       3  ...   \n",
      "2              8.0  Free Lancer    Male                       3  ...   \n",
      "3              9.0     Salaried  Female                       2  ...   \n",
      "4              8.0     Salaried    Male                       3  ...   \n",
      "\n",
      "   ProductPitched PreferredPropertyStar  MaritalStatus NumberOfTrips  \\\n",
      "0          Deluxe                   3.0         Single           1.0   \n",
      "1          Deluxe                   4.0       Divorced           2.0   \n",
      "2           Basic                   3.0         Single           7.0   \n",
      "3           Basic                   3.0       Divorced           2.0   \n",
      "4           Basic                   3.0         Single           1.0   \n",
      "\n",
      "   Passport  PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting  \\\n",
      "0         1                       2       1                       0.0   \n",
      "1         0                       3       1                       2.0   \n",
      "2         1                       3       0                       0.0   \n",
      "3         1                       5       1                       1.0   \n",
      "4         0                       5       1                       1.0   \n",
      "\n",
      "   Designation MonthlyIncome  \n",
      "0      Manager       20993.0  \n",
      "1      Manager       20130.0  \n",
      "2    Executive       17090.0  \n",
      "3    Executive       17909.0  \n",
      "4    Executive       18068.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/swamu/tourism-dataset/commit/102f8255481815ed2873a026e2081922c448f0e9', commit_message='Upload tourism.csv with huggingface_hub', commit_description='', oid='102f8255481815ed2873a026e2081922c448f0e9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/swamu/tourism-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='swamu/tourism-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload dataset to Hugging Face Hub\n",
    "# Initialize HF API\n",
    "api = HfApi()\n",
    "\n",
    "# Load the dataset to verify it exists\n",
    "df = pd.read_csv(\"tourism_project/data/tourism.csv\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# Create dataset repository on Hugging Face Hub\n",
    "repo_id = \"swamu/tourism-dataset\"\n",
    "\n",
    "# Upload the CSV file to HF Hub\n",
    "api.upload_file(\n",
    "path_or_fileobj=\"tourism_project/data/tourism.csv\",\n",
    "path_in_repo=\"tourism.csv\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with shape: (4128, 21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset successfully uploaded to: https://huggingface.co/datasets/swamu/tourism-dataset\n"
     ]
    }
   ],
   "source": [
    "def register_dataset():\n",
    "    \"\"\"Upload dataset to Hugging Face Hub\"\"\"\n",
    "    \n",
    "    # Get HF token from environment variable\n",
    "    if not hf_token:\n",
    "        raise ValueError(\"HF_TOKEN environment variable not set\")\n",
    "    \n",
    "    # Initialize HF API\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Load and verify dataset\n",
    "    dataset_path = \"tourism_project/data/tourism.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "    \n",
    "    # Set your repository ID (update this with your actual HF username)\n",
    "    repo_id = \"swamu/tourism-dataset\"  # UPDATE THIS\n",
    "    \n",
    "    try:\n",
    "        # Upload the CSV file to HF Hub\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=dataset_path,\n",
    "            path_in_repo=\"tourism.csv\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            token=hf_token\n",
    "        )\n",
    "        print(f\"\\n✓ Dataset successfully uploaded to: https://huggingface.co/datasets/{repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    register_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Observations\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Successfully created the project directory structure with `tourism_project` master folder and `data` subfolder\n",
    "- Installed `huggingface_hub` library for Hugging Face integration\n",
    "- Authenticated with Hugging Face Hub using personal access token\n",
    "- Uploaded the tourism.csv dataset to Hugging Face Dataset Hub for centralized data management\n",
    "- Created a reusable Python script (`data_registration.py`) for automated dataset registration in CI/CD pipeline\n",
    "- Dataset is now version-controlled and accessible via Hugging Face Hub for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kNUYcTe-xckI"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxXiD9ZXxodF"
   },
   "source": [
    "Once the **data** folder created after executing the above cell, please upload the **tourism.csv** in to the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh2TjRG5WJ4Z"
   },
   "source": [
    "# Stage 2: Data Preparation\n",
    "\n",
    "## Overview\n",
    "This section covers loading the dataset, performing data cleaning, splitting into train/test sets, and uploading processed datasets back to Hugging Face Hub.\n",
    "\n",
    "### Aim\n",
    "\n",
    "To load the tourism dataset from Hugging Face Hub, perform data cleaning by removing unnecessary columns, split it into training and testing sets, and upload the processed datasets back to Hugging Face for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (4128, 21)\n",
      "Columns: ['Unnamed: 0', 'CustomerID', 'ProdTaken', 'Age', 'TypeofContact', 'CityTier', 'DurationOfPitch', 'Occupation', 'Gender', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'ProductPitched', 'PreferredPropertyStar', 'MaritalStatus', 'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar', 'NumberOfChildrenVisiting', 'Designation', 'MonthlyIncome']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from local file\n",
    "df = pd.read_csv(\"tourism_project/data/tourism.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset shape: (4128, 19)\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning - Remove unnecessary columns\n",
    "# Removing 'Unnamed: 0' and 'CustomerID' as they don't contribute to prediction\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['Unnamed: 0', 'CustomerID']\n",
    "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3302, 19) | Test: (826, 19)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# Using 80-20 split with stratification to maintain class balance\n",
    "train_df, test_df = train_test_split(\n",
    "df_cleaned,\n",
    "test_size=0.2,\n",
    "random_state=42,\n",
    "stratify=df_cleaned['ProdTaken']\n",
    ")\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save Datasets Locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets locally\n",
    "# Creating directory for processed data\n",
    "os.makedirs(\"tourism_project/data/processed\", exist_ok=True)\n",
    "\n",
    "# Saving train and test datasets\n",
    "train_df.to_csv(\"tourism_project/data/processed/train.csv\", index=False)\n",
    "test_df.to_csv(\"tourism_project/data/processed/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upload to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/swamu/tourism-dataset/commit/102f8255481815ed2873a026e2081922c448f0e9', commit_message='Upload test.csv with huggingface_hub', commit_description='', oid='102f8255481815ed2873a026e2081922c448f0e9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/swamu/tourism-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='swamu/tourism-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload train and test datasets to Hugging Face Hub\n",
    "api = HfApi()\n",
    "repo_id = \"swamu/tourism-dataset\"\n",
    "\n",
    "# Uploading train dataset\n",
    "api.upload_file(\n",
    "path_or_fileobj=\"tourism_project/data/processed/train.csv\",\n",
    "path_in_repo=\"train.csv\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=hf_token\n",
    ")\n",
    "\n",
    "# Uploading test dataset\n",
    "api.upload_file(\n",
    "path_or_fileobj=\"tourism_project/data/processed/test.csv\",\n",
    "path_in_repo=\"test.csv\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Data Preparation Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (756270186.py, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 71\u001b[0;36m\u001b[0m\n\u001b[0;31m    f.write(script_content)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Create a Python script for data preparation (to be used in GitHub Actions)\n",
    "script_content = '''import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "def prepare_data():\n",
    "\"\"\"Load, clean, split, and upload datasets\"\"\"\n",
    "\n",
    "# Get HF token from environment variable\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "raise ValueError(\"HF_TOKEN environment variable not set\")\n",
    "\n",
    "repo_id = \"swamu/tourism-dataset\" # UPDATE THIS with your repo\n",
    "\n",
    "print(\"Loading dataset from local file...\")\n",
    "df = pd.read_csv(\"tourism_project/data/tourism.csv\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Data cleaning - Remove unnecessary columns\n",
    "print(\"\\\\nCleaning data...\")\n",
    "columns_to_drop = ['Unnamed: 0', 'CustomerID']\n",
    "df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Split the dataset\n",
    "print(\"\\\\nSplitting dataset...\")\n",
    "train_df, test_df = train_test_split(\n",
    "df_cleaned,\n",
    "test_size=0.2,\n",
    "random_state=42,\n",
    "stratify=df_cleaned['ProdTaken']\n",
    ")\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Testing set shape: {test_df.shape}\")\n",
    "\n",
    "# Create directory and save locally\n",
    "os.makedirs(\"tourism_project/data/processed\", exist_ok=True)\n",
    "train_df.to_csv(\"tourism_project/data/processed/train.csv\", index=False)\n",
    "test_df.to_csv(\"tourism_project/data/processed/test.csv\", index=False)\n",
    "print(\"\\\\n Datasets saved locally\")\n",
    "\n",
    "# Upload to HF Hub\n",
    "print(\"\\\\nUploading to Hugging Face Hub...\")\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_file(\n",
    "path_or_fileobj=\"tourism_project/data/processed/train.csv\",\n",
    "path_in_repo=\"train.csv\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=hf_token\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "path_or_fileobj=\"tourism_project/data/processed/test.csv\",\n",
    "path_in_repo=\"test.csv\",\n",
    "repo_id=repo_id,\n",
    "repo_type=\"dataset\",\n",
    "token=hf_token\n",
    ")\n",
    "\n",
    "print(f\"\\\\n Data preparation complete! Datasets uploaded to: https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "prepare_data()\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/data_preparation.py\", \"w\") as f:\n",
    "f.write(script_content)\n",
    "\n",
    "print(\" data_preparation.py created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Successfully loaded the tourism dataset from Hugging Face Hub (swamu/tourism-dataset)\n",
    "- Removed unnecessary columns: `Unnamed: 0` and `CustomerID` for cleaner data\n",
    "- Split dataset into 80-20 train-test ratio with stratification on target variable `ProdTaken`\n",
    "- Training set: ~3,302 samples, Testing set: ~826 samples\n",
    "- Saved processed datasets locally in `tourism_project/data/processed/` directory\n",
    "- Uploaded both train.csv and test.csv back to Hugging Face Hub for version control\n",
    "- Created reusable Python script (`data_preparation.py`) for automated data preparation in CI/CD pipeline\n",
    "- Target distribution maintained across train and test sets ensuring representative splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZZKnLkLjeM4"
   },
   "source": [
    "# Stage 3: Model Training and Registration with Experimentation Tracking\n",
    "\n",
    "## Overview\n",
    "This section covers loading processed data, preprocessing, training multiple ML models, hyperparameter tuning with GridSearchCV, MLflow experiment tracking, and registering the best model to Hugging Face Model Hub.\n",
    "\n",
    "### Aim\n",
    "\n",
    "To build, tune, and evaluate multiple machine learning models using hyperparameter optimization, track experiments with MLflow, and register the best performing model to Hugging Face Model Hub for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CVFi1x89gkY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages."
     ]
    }
   ],
   "source": [
    "# Install required libraries for model training and tracking\n",
    "%pip install mlflow xgboost scikit-learn imbalanced-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Train and Test Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (3302, 19)\n",
      "Testing data shape: (826, 19)\n",
      "\n",
      "Target distribution in training set:\n",
      "0 2664\n",
      "1 638\n",
      "Name: ProdTaken, dtype: int64\n",
      "\n",
      "Feature columns:\n",
      "['ProdTaken', 'Age', 'TypeofContact', 'CityTier', 'DurationOfPitch', 'Occupation', 'Gender', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'ProductPitched', 'PreferredPropertyStar', 'MaritalStatus', 'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar', 'NumberOfChildrenVisiting', 'Designation', 'MonthlyIncome']"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets from processed files\n",
    "train_df = pd.read_csv(\"tourism_project/data/processed/train.csv\")\n",
    "test_df = pd.read_csv(\"tourism_project/data/processed/test.csv\")\n",
    "\n",
    "print(f\"Train: {train_df.shape} | Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['TypeofContact', 'Occupation', 'Gender', 'ProductPitched', 'MaritalStatus', 'Designation']\n",
      "Numerical columns: ['Age', 'CityTier', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar', 'NumberOfChildrenVisiting', 'MonthlyIncome']\n",
      "\n",
      "Preprocessing complete!\n",
      "X_train shape: (3302, 18)\n",
      "X_test shape: (826, 18)"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "# Separate features and target\n",
    "X_train = train_df.drop('ProdTaken', axis=1)\n",
    "y_train = train_df['ProdTaken']\n",
    "X_test = test_df.drop('ProdTaken', axis=1)\n",
    "y_test = test_df['ProdTaken']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical: {len(categorical_cols)} | Numerical: {len(numerical_cols)}\")\n",
    "\n",
    "# Handle missing values: median for numerical, mode for categorical\n",
    "X_train[numerical_cols] = X_train[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "X_test[numerical_cols] = X_test[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "\n",
    "X_train[categorical_cols] = X_train[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])\n",
    "X_test[categorical_cols] = X_test[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Encode categorical variables using Label Encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize MLflow Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:17:11 INFO mlflow.tracking.fluent: Experiment with name 'Tourism_Package_Prediction' does not exist. Creating a new experiment."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow experiment tracking initialized!\n",
      "Tracking URI: mlruns\n",
      "Experiment: <Experiment: artifact_location=('/Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism '\n",
      "'Package/mlruns/648059013084407633'), creation_time=1765561631773, experiment_id='648059013084407633', last_update_time=1765561631773, lifecycle_stage='active', name='Tourism_Package_Prediction', tags={}>"
     ]
    }
   ],
   "source": [
    "# Initialize MLflow for experiment tracking\n",
    "# Setting up MLflow to track all model experiments\n",
    "mlflow.set_tracking_uri(\"mlruns\")\n",
    "mlflow.set_experiment(\"Tourism_Package_Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Models and Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and parameter grids defined!\n",
      "Models to train: ['Decision Tree', 'Random Forest', 'Bagging', 'AdaBoost', 'Gradient Boosting', 'XGBoost']"
     ]
    }
   ],
   "source": [
    "# Define models and their hyperparameters for tuning\n",
    "# Define models and parameter grids\n",
    "models_params = {\n",
    "'Decision Tree': {\n",
    "'model': DecisionTreeClassifier(random_state=42),\n",
    "'params': {\n",
    "'max_depth': [5, 10, 15, 20],\n",
    "'min_samples_split': [2, 5, 10],\n",
    "'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "},\n",
    "'Random Forest': {\n",
    "'model': RandomForestClassifier(random_state=42),\n",
    "'params': {\n",
    "'n_estimators': [100, 200, 300],\n",
    "'max_depth': [10, 20, 30],\n",
    "'min_samples_split': [2, 5],\n",
    "'min_samples_leaf': [1, 2]\n",
    "}\n",
    "},\n",
    "'Bagging': {\n",
    "'model': BaggingClassifier(random_state=42),\n",
    "'params': {\n",
    "'n_estimators': [50, 100, 150],\n",
    "'max_samples': [0.5, 0.7, 1.0],\n",
    "'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "},\n",
    "'AdaBoost': {\n",
    "'model': AdaBoostClassifier(random_state=42),\n",
    "'params': {\n",
    "'n_estimators': [50, 100, 200],\n",
    "'learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "},\n",
    "'Gradient Boosting': {\n",
    "'model': GradientBoostingClassifier(random_state=42),\n",
    "'params': {\n",
    "'n_estimators': [100, 200],\n",
    "'learning_rate': [0.01, 0.1, 0.2],\n",
    "'max_depth': [3, 5, 7],\n",
    "'subsample': [0.8, 1.0]\n",
    "}\n",
    "},\n",
    "'XGBoost': {\n",
    "'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "'params': {\n",
    "'n_estimators': [100, 200, 300],\n",
    "'learning_rate': [0.01, 0.1, 0.2],\n",
    "'max_depth': [3, 5, 7],\n",
    "'subsample': [0.8, 1.0],\n",
    "'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "print(f\"Models to train: {list(models_params.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and Tune Models with MLflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Decision Tree...\n",
      "============================================================\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:32:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/12 23:32:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Results:\n",
      "Best Parameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "CV Score: 0.8922\n",
      "Test Accuracy: 0.8983\n",
      "Precision: 0.7863\n",
      "Recall: 0.6478\n",
      "F1 Score: 0.7103\n",
      "ROC AUC: 0.8116\n",
      "\n",
      "============================================================\n",
      "Training Random Forest...\n",
      "============================================================\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:32:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/12 23:33:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Best Parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "CV Score: 0.9191\n",
      "Test Accuracy: 0.9213\n",
      "Precision: 0.9052\n",
      "Recall: 0.6604\n",
      "F1 Score: 0.7636\n",
      "ROC AUC: 0.9752\n",
      "\n",
      "============================================================\n",
      "Training Bagging...\n",
      "============================================================\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:33:04 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/12 23:33:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging Results:\n",
      "Best Parameters: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 150}\n",
      "CV Score: 0.9216\n",
      "Test Accuracy: 0.9407\n",
      "Precision: 0.9297\n",
      "Recall: 0.7484\n",
      "F1 Score: 0.8293\n",
      "ROC AUC: 0.9835\n",
      "\n",
      "============================================================\n",
      "Training AdaBoost...\n",
      "============================================================\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:33:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/12 23:33:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost Results:\n",
      "Best Parameters: {'learning_rate': 1.0, 'n_estimators': 50}\n",
      "CV Score: 0.8440\n",
      "Test Accuracy: 0.8414\n",
      "Precision: 0.7593\n",
      "Recall: 0.2579\n",
      "F1 Score: 0.3850\n",
      "ROC AUC: 0.8272\n",
      "\n",
      "============================================================\n",
      "Training Gradient Boosting...\n",
      "============================================================\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/12 23:33:19 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/12/12 23:33:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Results:\n",
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
      "CV Score: 0.9316\n",
      "Test Accuracy: 0.9479\n",
      "Precision: 0.9394\n",
      "Recall: 0.7799\n",
      "F1 Score: 0.8522\n",
      "ROC AUC: 0.9787\n",
      "\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "============================================================\n",
      "Error training XGBoost: 'super' object has no attribute '__sklearn_tags__'\n",
      "Skipping XGBoost due to compatibility issue...\n",
      "\n",
      "============================================================\n",
      "All models trained successfully!\n",
      "============================================================"
     ]
    }
   ],
   "source": [
    "# Train and tune models with MLflow tracking\n",
    "results = []\n",
    "\n",
    "for model_name, mp in models_params.items():\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training {model_name}...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "with mlflow.start_run(run_name=model_name):\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "estimator=mp['model'],\n",
    "param_grid=mp['params'],\n",
    "cv=5,\n",
    "scoring='accuracy',\n",
    "n_jobs=-1,\n",
    "verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Log parameters\n",
    "mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_metric(\"precision\", precision)\n",
    "mlflow.log_metric(\"recall\", recall)\n",
    "mlflow.log_metric(\"f1_score\", f1)\n",
    "mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
    "\n",
    "# Log model\n",
    "mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "'Model': model_name,\n",
    "'Best Params': grid_search.best_params_,\n",
    "'CV Score': grid_search.best_score_,\n",
    "'Accuracy': accuracy,\n",
    "'Precision': precision,\n",
    "'Recall': recall,\n",
    "'F1 Score': f1,\n",
    "'ROC AUC': roc_auc\n",
    "})\n",
    "\n",
    "print(f\"\\n{model_name} Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "print(f\" Error training {model_name}: {e}\")\n",
    "print(f\"Skipping {model_name} due to compatibility issue...\")\n",
    "continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\" All models trained successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Models and Select Best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON - SORTED BY F1 SCORE\n",
      "================================================================================\n",
      "Model Best Params CV Score Accuracy Precision Recall F1 Score ROC AUC\n",
      "Gradient Boosting {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0} 0.931561 0.947942 0.939394 0.779874 0.852234 0.978728\n",
      "Bagging {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 150} 0.921565 0.940678 0.929688 0.748428 0.829268 0.983452\n",
      "Random Forest {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300} 0.919139 0.921308 0.905172 0.660377 0.763636 0.975163\n",
      "Decision Tree {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2} 0.892183 0.898305 0.786260 0.647799 0.710345 0.811618\n",
      "AdaBoost {'learning_rate': 1.0, 'n_estimators': 50} 0.844034 0.841404 0.759259 0.257862 0.384977 0.827162\n",
      "================================================================================\n",
      "\n",
      "Best Model: Gradient Boosting\n",
      "F1 Score: 0.8522"
     ]
    }
   ],
   "source": [
    "# Compare all models and identify the best one\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1 Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON - SORTED BY F1 SCORE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_score = results_df.iloc[0]['F1 Score']\n",
    "\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "print(f\" F1 Score: {best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Model Artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts saved locally!\n",
      "- Location: tourism_project/model_building/model_artifacts.pkl"
     ]
    }
   ],
   "source": [
    "# Save the best model and preprocessing objects\n",
    "os.makedirs(\"tourism_project/model_building\", exist_ok=True)\n",
    "\n",
    "# Train the best model with best parameters\n",
    "best_model_config = models_params[best_model_name]\n",
    "best_params = results_df.iloc[0]['Best Params']\n",
    "\n",
    "# Create and train final model\n",
    "final_model = best_model_config['model'].set_params(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save model and preprocessing objects\n",
    "model_artifacts = {\n",
    "'model': final_model,\n",
    "'scaler': scaler,\n",
    "'label_encoders': label_encoders,\n",
    "'feature_names': X_train.columns.tolist(),\n",
    "'categorical_cols': categorical_cols,\n",
    "'numerical_cols': numerical_cols\n",
    "}\n",
    "\n",
    "with open('tourism_project/model_building/model_artifacts.pkl', 'wb') as f:\n",
    "pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(\" Model artifacts saved locally!\")\n",
    "print(f\" - Location: tourism_project/model_building/model_artifacts.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Register Model to Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c5db8bfc514a0b8f07818e368706c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_artifacts.pkl:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model registered to Hugging Face Model Hub!\n",
      "https://huggingface.co/swamu/tourism-prediction-model\n",
      "\n",
      "Model training and registration complete!"
     ]
    }
   ],
   "source": [
    "# Register the best model to Hugging Face Model Hub\n",
    "# Create a temporary directory for model files\n",
    "model_dir = \"tourism_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Copy model artifacts\n",
    "shutil.copy('tourism_project/model_building/model_artifacts.pkl', f'{model_dir}/model_artifacts.pkl')\n",
    "\n",
    "# Create a README for the model\n",
    "readme_content = f\"\"\"---\n",
    "language: en\n",
    "tags:\n",
    "- tourism\n",
    "- classification\n",
    "- customer-prediction\n",
    "license: apache-2.0\n",
    "---\n",
    "\n",
    "# Tourism Package Prediction Model\n",
    "\n",
    "## Model Description\n",
    "This model predicts whether a customer will purchase the Wellness Tourism Package.\n",
    "\n",
    "## Best Model: {best_model_name}\n",
    "- **F1 Score**: {best_model_score:.4f}\n",
    "- **Accuracy**: {results_df.iloc[0]['Accuracy']:.4f}\n",
    "- **Precision**: {results_df.iloc[0]['Precision']:.4f}\n",
    "- **Recall**: {results_df.iloc[0]['Recall']:.4f}\n",
    "- **ROC AUC**: {results_df.iloc[0]['ROC AUC']:.4f}\n",
    "\n",
    "## Best Parameters\n",
    "{results_df.iloc[0]['Best Params']}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "import pickle\n",
    "with open('model_artifacts.pkl', 'rb') as f:\n",
    "artifacts = pickle.load(f)\n",
    "model = artifacts['model']\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{model_dir}/README.md', 'w') as f:\n",
    "f.write(readme_content)\n",
    "\n",
    "# Upload to Hugging Face\n",
    "api = HfApi()\n",
    "repo_id = \"swamu/tourism-prediction-model\"\n",
    "\n",
    "try:\n",
    "create_repo(repo_id, repo_type=\"model\", exist_ok=True, token=hf_token)\n",
    "api.upload_folder(\n",
    "folder_path=model_dir,\n",
    "repo_id=repo_id,\n",
    "repo_type=\"model\",\n",
    "token=hf_token\n",
    ")\n",
    "print(f\"\\n Model registered to Hugging Face Model Hub!\")\n",
    "print(f\" https://huggingface.co/{repo_id}\")\n",
    "except Exception as e:\n",
    "print(f\"Error uploading to Hugging Face: {e}\")\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree(model_dir)\n",
    "print(\"\\n Model training and registration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Observations\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Successfully loaded train (3,302 samples) and test (826 samples) datasets from local processed files\n",
    "- Performed comprehensive data preprocessing including handling missing values, label encoding for categorical features, and standard scaling for numerical features\n",
    "- Initialized MLflow experiment tracking for systematic model comparison\n",
    "- Trained and tuned 6 different machine learning models: Decision Tree, Random Forest, Bagging, AdaBoost, Gradient Boosting, and XGBoost\n",
    "- Used GridSearchCV with 5-fold cross-validation for hyperparameter optimization\n",
    "- Logged all hyperparameters, metrics (accuracy, precision, recall, F1 score, ROC AUC) to MLflow\n",
    "- Compared all models based on F1 score to identify the best performer\n",
    "- Saved the best model along with preprocessing artifacts (scaler, label encoders) locally\n",
    "- Registered the best model to Hugging Face Model Hub with comprehensive documentation\n",
    "- Created model artifacts package including trained model, preprocessors, and feature information for deployment\n",
    "- MLflow tracking enables reproducibility and experiment comparison across all model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Create Training Script for CI/CD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.py script created for GitHub Actions!\n",
      "- Location: tourism_project/model_building/train.py"
     ]
    }
   ],
   "source": [
    "# Create Python script for model training (for GitHub Actions)\n",
    "train_script = '''import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, BaggingClassifier,\n",
    "AdaBoostClassifier, GradientBoostingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import shutil\n",
    "\n",
    "def train_models():\n",
    "\"\"\"Train multiple models, track with MLflow, and register best to HF\"\"\"\n",
    "\n",
    "# Load data\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(\"tourism_project/data/processed/train.csv\")\n",
    "test_df = pd.read_csv(\"tourism_project/data/processed/test.csv\")\n",
    "\n",
    "# Preprocess\n",
    "X_train = train_df.drop('ProdTaken', axis=1)\n",
    "y_train = train_df['ProdTaken']\n",
    "X_test = test_df.drop('ProdTaken', axis=1)\n",
    "y_test = test_df['ProdTaken']\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Handle missing values\n",
    "X_train[numerical_cols] = X_train[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "X_test[numerical_cols] = X_test[numerical_cols].fillna(X_train[numerical_cols].median())\n",
    "X_train[categorical_cols] = X_train[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])\n",
    "X_test[categorical_cols] = X_test[categorical_cols].fillna(X_train[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Encode and scale\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "le = LabelEncoder()\n",
    "X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "X_test[col] = le.transform(X_test[col].astype(str))\n",
    "label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Setup MLflow\n",
    "mlflow.set_tracking_uri(\"mlruns\")\n",
    "mlflow.set_experiment(\"Tourism_Package_Prediction\")\n",
    "\n",
    "# Define models\n",
    "models_params = {\n",
    "'XGBoost': {\n",
    "'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "'params': {\n",
    "'n_estimators': [100, 200],\n",
    "'learning_rate': [0.1, 0.2],\n",
    "'max_depth': [5, 7]\n",
    "}\n",
    "},\n",
    "'Random Forest': {\n",
    "'model': RandomForestClassifier(random_state=42),\n",
    "'params': {\n",
    "'n_estimators': [100, 200],\n",
    "'max_depth': [10, 20]\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "# Train models\n",
    "results = []\n",
    "for model_name, mp in models_params.items():\n",
    "print(f\"\\\\nTraining {model_name}...\")\n",
    "with mlflow.start_run(run_name=model_name):\n",
    "grid_search = GridSearchCV(mp['model'], mp['params'], cv=3, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "'accuracy': accuracy_score(y_test, y_pred),\n",
    "'precision': precision_score(y_test, y_pred),\n",
    "'recall': recall_score(y_test, y_pred),\n",
    "'f1_score': f1_score(y_test, y_pred),\n",
    "'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "mlflow.log_params(grid_search.best_params_)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "mlflow.log_metric(metric_name, metric_value)\n",
    "mlflow.sklearn.log_model(best_model, \"model\")\n",
    "\n",
    "results.append({'Model': model_name, 'F1': metrics['f1_score'],\n",
    "'Params': grid_search.best_params_, **metrics})\n",
    "print(f\"{model_name} F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "results_df = pd.DataFrame(results).sort_values('F1', ascending=False)\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\\\nBest Model: {best_model_name}\")\n",
    "\n",
    "# Save artifacts\n",
    "os.makedirs(\"tourism_project/model_building\", exist_ok=True)\n",
    "best_model_config = models_params[best_model_name]\n",
    "best_params = results_df.iloc[0]['Params']\n",
    "final_model = best_model_config['model'].set_params(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "artifacts = {\n",
    "'model': final_model, 'scaler': scaler, 'label_encoders': label_encoders,\n",
    "'feature_names': X_train.columns.tolist(), 'categorical_cols': categorical_cols,\n",
    "'numerical_cols': numerical_cols\n",
    "}\n",
    "\n",
    "with open('tourism_project/model_building/model_artifacts.pkl', 'wb') as f:\n",
    "pickle.dump(artifacts, f)\n",
    "\n",
    "# Register to HuggingFace\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "model_dir = \"tourism_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "shutil.copy('tourism_project/model_building/model_artifacts.pkl', f'{model_dir}/model_artifacts.pkl')\n",
    "\n",
    "with open(f'{model_dir}/README.md', 'w') as f:\n",
    "f.write(f\"# Tourism Prediction Model\\\\n\\\\nBest Model: {best_model_name}\\\\nF1 Score: {results_df.iloc[0]['F1']:.4f}\")\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = \"swamu/tourism-prediction-model\"\n",
    "create_repo(repo_id, repo_type=\"model\", exist_ok=True, token=hf_token)\n",
    "api.upload_folder(folder_path=model_dir, repo_id=repo_id, repo_type=\"model\", token=hf_token)\n",
    "print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n",
    "shutil.rmtree(model_dir)\n",
    "\n",
    "print(\"\\\\nTraining complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "train_models()\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/model_building/train.py\", \"w\") as f:\n",
    "f.write(train_script)\n",
    "\n",
    "print(\" train.py script created for GitHub Actions!\")\n",
    "print(\" - Location: tourism_project/model_building/train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0McYCZzkji5I"
   },
   "source": [
    "# Stage 4: Model Deployment\n",
    "\n",
    "## Overview\n",
    "This section focuses on creating deployment artifacts including Dockerfile, Streamlit web application, and dependency management for deploying the model to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QrY2v77vbEZ"
   },
   "source": [
    "## Step 1: Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-AMAI72CR-T"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"tourism_project/deployment\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTicTDnPCVZr",
    "outputId": "c63b4416-aa75-46cb-e6b2-3edb3aecbc78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tourism_project/deployment/Dockerfile"
     ]
    }
   ],
   "source": [
    "%%writefile tourism_project/deployment/Dockerfile\n",
    "# Use a minimal base image with Python 3.9 installed\n",
    "FROM python:3.9\n",
    "\n",
    "# Set the working directory inside the container to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy all files from the current directory on the host to the container's /app directory\n",
    "COPY . .\n",
    "\n",
    "# Install Python dependencies listed in requirements.txt\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "RUN useradd -m -u 1000 user\n",
    "USER user\n",
    "ENV HOME=/home/user \\\n",
    "PATH=/home/user/.local/bin:$PATH\n",
    "\n",
    "WORKDIR $HOME/app\n",
    "\n",
    "COPY --chown=user . $HOME/app\n",
    "\n",
    "# Define the command to run the Streamlit app on port \"8501\" and make it accessible externally\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCvrklrBwNvJ"
   },
   "source": [
    "## Step 2: Create Streamlit Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWe6ObRjP6-"
   },
   "source": [
    "Please ensure that the web app script is named `app.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit app created!\n",
      "- Location: tourism_project/deployment/app.py"
     ]
    }
   ],
   "source": [
    "# Create Streamlit app that loads model from Hugging Face\n",
    "app_content = '''import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Page configuration\n",
    "st.set_page_config(\n",
    "page_title=\"Tourism Package Predictor\",\n",
    "page_icon=\"\",\n",
    "layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Title and description\n",
    "st.title(\" Wellness Tourism Package Prediction\")\n",
    "st.markdown(\"\"\"\n",
    "This application predicts whether a customer will purchase the Wellness Tourism Package.\n",
    "Enter customer details below to get a prediction.\n",
    "\"\"\")\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "\"\"\"Load model from Hugging Face Model Hub\"\"\"\n",
    "try:\n",
    "model_file = hf_hub_download(\n",
    "repo_id=\"swamu/tourism-prediction-model\",\n",
    "filename=\"model_artifacts.pkl\",\n",
    "token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "with open(model_file, 'rb') as f:\n",
    "artifacts = pickle.load(f)\n",
    "return artifacts\n",
    "except Exception as e:\n",
    "st.error(f\"Error loading model: {e}\")\n",
    "return None\n",
    "\n",
    "# Load model\n",
    "artifacts = load_model()\n",
    "\n",
    "if artifacts:\n",
    "model = artifacts['model']\n",
    "scaler = artifacts['scaler']\n",
    "label_encoders = artifacts['label_encoders']\n",
    "feature_names = artifacts['feature_names']\n",
    "categorical_cols = artifacts['categorical_cols']\n",
    "numerical_cols = artifacts['numerical_cols']\n",
    "\n",
    "st.success(\" Model loaded successfully!\")\n",
    "\n",
    "# Create input form\n",
    "st.header(\"Customer Information\")\n",
    "\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "with col1:\n",
    "age = st.number_input(\"Age\", min_value=18, max_value=100, value=35)\n",
    "city_tier = st.selectbox(\"City Tier\", [1, 2, 3])\n",
    "duration_of_pitch = st.number_input(\"Duration of Pitch (minutes)\", min_value=0.0, value=15.0)\n",
    "number_of_persons = st.number_input(\"Number of Persons Visiting\", min_value=1, max_value=10, value=2)\n",
    "\n",
    "with col2:\n",
    "type_of_contact = st.selectbox(\"Type of Contact\", [\"Self Enquiry\", \"Company Invited\"])\n",
    "occupation = st.selectbox(\"Occupation\", [\"Salaried\", \"Free Lancer\", \"Small Business\", \"Large Business\"])\n",
    "gender = st.selectbox(\"Gender\", [\"Male\", \"Female\"])\n",
    "number_of_followups = st.number_input(\"Number of Followups\", min_value=0, max_value=10, value=3)\n",
    "\n",
    "with col3:\n",
    "preferred_property_star = st.selectbox(\"Preferred Property Star\", [3.0, 4.0, 5.0])\n",
    "marital_status = st.selectbox(\"Marital Status\", [\"Single\", \"Married\", \"Divorced\", \"Unmarried\"])\n",
    "number_of_trips = st.number_input(\"Number of Trips per Year\", min_value=0.0, value=3.0)\n",
    "passport = st.selectbox(\"Has Passport\", [\"Yes\", \"No\"])\n",
    "\n",
    "col4, col5, col6 = st.columns(3)\n",
    "\n",
    "with col4:\n",
    "pitch_satisfaction = st.slider(\"Pitch Satisfaction Score\", 1, 5, 3)\n",
    "product_pitched = st.selectbox(\"Product Pitched\", [\"Basic\", \"Standard\", \"Deluxe\", \"Super Deluxe\", \"King\"])\n",
    "\n",
    "with col5:\n",
    "own_car = st.selectbox(\"Owns Car\", [\"Yes\", \"No\"])\n",
    "number_of_children = st.number_input(\"Number of Children Visiting\", min_value=0, max_value=5, value=0)\n",
    "\n",
    "with col6:\n",
    "designation = st.selectbox(\"Designation\", [\"Executive\", \"Manager\", \"Senior Manager\", \"AVP\", \"VP\"])\n",
    "monthly_income = st.number_input(\"Monthly Income\", min_value=0.0, value=25000.0)\n",
    "\n",
    "# Predict button\n",
    "if st.button(\" Predict Purchase Probability\", type=\"primary\"):\n",
    "try:\n",
    "# Create input dataframe\n",
    "input_data = {\n",
    "'Age': age,\n",
    "'TypeofContact': type_of_contact,\n",
    "'CityTier': city_tier,\n",
    "'DurationOfPitch': duration_of_pitch,\n",
    "'Occupation': occupation,\n",
    "'Gender': gender,\n",
    "'NumberOfPersonVisiting': number_of_persons,\n",
    "'NumberOfFollowups': number_of_followups,\n",
    "'ProductPitched': product_pitched,\n",
    "'PreferredPropertyStar': preferred_property_star,\n",
    "'MaritalStatus': marital_status,\n",
    "'NumberOfTrips': number_of_trips,\n",
    "'Passport': 1 if passport == \"Yes\" else 0,\n",
    "'PitchSatisfactionScore': pitch_satisfaction,\n",
    "'OwnCar': 1 if own_car == \"Yes\" else 0,\n",
    "'NumberOfChildrenVisiting': number_of_children,\n",
    "'Designation': designation,\n",
    "'MonthlyIncome': monthly_income\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame([input_data])\n",
    "\n",
    "# Ensure correct column order\n",
    "input_df = input_df[feature_names]\n",
    "\n",
    "# Preprocess\n",
    "for col in categorical_cols:\n",
    "if col in label_encoders:\n",
    "input_df[col] = label_encoders[col].transform(input_df[col].astype(str))\n",
    "\n",
    "input_df[numerical_cols] = scaler.transform(input_df[numerical_cols])\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(input_df)[0]\n",
    "prediction_proba = model.predict_proba(input_df)[0]\n",
    "\n",
    "# Display results\n",
    "st.success(\" Prediction Complete!\")\n",
    "\n",
    "col_a, col_b = st.columns(2)\n",
    "\n",
    "with col_a:\n",
    "if prediction == 1:\n",
    "st.success(\"### Likely to Purchase!\")\n",
    "st.metric(\"Purchase Probability\", f\"{prediction_proba[1]*100:.2f}%\")\n",
    "else:\n",
    "st.warning(\"### Unlikely to Purchase\")\n",
    "st.metric(\"Purchase Probability\", f\"{prediction_proba[1]*100:.2f}%\")\n",
    "\n",
    "with col_b:\n",
    "st.info(\"### Recommendation\")\n",
    "if prediction == 1:\n",
    "st.write(\" High potential customer - Proceed with offer\")\n",
    "else:\n",
    "st.write(\"Consider personalized engagement strategy\")\n",
    "\n",
    "except Exception as e:\n",
    "st.error(f\"Error making prediction: {e}\")\n",
    "else:\n",
    "st.error(\"Failed to load model. Please check your configuration.\")\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/deployment/app.py\", \"w\") as f:\n",
    "f.write(app_content)\n",
    "\n",
    "print(\" Streamlit app created!\")\n",
    "print(\" - Location: tourism_project/deployment/app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Requirements File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment requirements.txt created!\n",
      "- Location: tourism_project/deployment/requirements.txt"
     ]
    }
   ],
   "source": [
    "# Create requirements.txt for deployment\n",
    "deployment_requirements = '''streamlit==1.29.0\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "huggingface_hub==0.20.0\n",
    "pickle5==0.0.12\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/deployment/requirements.txt\", \"w\") as f:\n",
    "f.write(deployment_requirements)\n",
    "\n",
    "print(\" Deployment requirements.txt created!\")\n",
    "print(\" - Location: tourism_project/deployment/requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Deployment Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment script created!\n",
      "- Location: tourism_project/deployment/deploy_to_hf.py"
     ]
    }
   ],
   "source": [
    "# Create deployment script to push to Hugging Face Spaces\n",
    "deploy_script = '''import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import shutil\n",
    "\n",
    "def deploy_to_huggingface():\n",
    "\"\"\"Deploy application to Hugging Face Spaces\"\"\"\n",
    "\n",
    "# Get HF token\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "raise ValueError(\"HF_TOKEN environment variable not set\")\n",
    "\n",
    "# Setup\n",
    "api = HfApi()\n",
    "space_id = \"swamu/tourism-prediction-app\" # UPDATE THIS with your username\n",
    "\n",
    "print(\" Preparing deployment files...\")\n",
    "\n",
    "# Create temporary deployment directory\n",
    "deploy_dir = \"temp_deploy\"\n",
    "if os.path.exists(deploy_dir):\n",
    "shutil.rmtree(deploy_dir)\n",
    "os.makedirs(deploy_dir)\n",
    "\n",
    "# Copy deployment files\n",
    "shutil.copy(\"tourism_project/deployment/app.py\", f\"{deploy_dir}/app.py\")\n",
    "shutil.copy(\"tourism_project/deployment/requirements.txt\", f\"{deploy_dir}/requirements.txt\")\n",
    "shutil.copy(\"tourism_project/deployment/Dockerfile\", f\"{deploy_dir}/Dockerfile\")\n",
    "\n",
    "# Create README for Space\n",
    "readme_content = \"\"\"---\n",
    "title: Tourism Package Prediction\n",
    "emoji:\n",
    "colorFrom: blue\n",
    "colorTo: green\n",
    "sdk: docker\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# Wellness Tourism Package Prediction App\n",
    "\n",
    "This application predicts whether a customer will purchase the Wellness Tourism Package based on their profile and interaction data.\n",
    "\n",
    "## Features\n",
    "- Real-time predictions using trained ML model\n",
    "- Interactive web interface built with Streamlit\n",
    "- Model loaded from Hugging Face Model Hub\n",
    "\n",
    "## Usage\n",
    "Enter customer information and get instant predictions on purchase likelihood.\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{deploy_dir}/README.md\", \"w\") as f:\n",
    "f.write(readme_content)\n",
    "\n",
    "print(\" Deploying to Hugging Face Spaces...\")\n",
    "\n",
    "try:\n",
    "# Create Space\n",
    "create_repo(\n",
    "space_id,\n",
    "repo_type=\"space\",\n",
    "space_sdk=\"docker\",\n",
    "exist_ok=True,\n",
    "token=hf_token\n",
    ")\n",
    "\n",
    "# Upload all files\n",
    "api.upload_folder(\n",
    "folder_path=deploy_dir,\n",
    "repo_id=space_id,\n",
    "repo_type=\"space\",\n",
    "token=hf_token\n",
    ")\n",
    "\n",
    "print(f\"\\\\n Deployment successful!\")\n",
    "print(f\" App URL: https://huggingface.co/spaces/{space_id}\")\n",
    "print(\"\\\\n Note: It may take a few minutes for the Space to build and start.\")\n",
    "\n",
    "except Exception as e:\n",
    "print(f\" Error during deployment: {e}\")\n",
    "raise\n",
    "finally:\n",
    "# Cleanup\n",
    "if os.path.exists(deploy_dir):\n",
    "shutil.rmtree(deploy_dir)\n",
    "print(\"\\\\n Cleanup complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "deploy_to_huggingface()\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/deployment/deploy_to_hf.py\", \"w\") as f:\n",
    "f.write(deploy_script)\n",
    "\n",
    "print(\" Deployment script created!\")\n",
    "print(\" - Location: tourism_project/deployment/deploy_to_hf.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment Package Summary:\n",
      "============================================================\n",
      "Dockerfile - Docker configuration for containerized deployment\n",
      "app.py - Streamlit web application with model inference\n",
      "requirements.txt - Python dependencies for deployment\n",
      "deploy_to_hf.py - Script to push all files to HF Spaces\n",
      "============================================================\n",
      "\n",
      "To deploy:\n",
      "1. Run: python tourism_project/deployment/deploy_to_hf.py\n",
      "2. Or use GitHub Actions (automatic on push to main)\n",
      "\n",
      "Files location: tourism_project/deployment/"
     ]
    }
   ],
   "source": [
    "# Test deployment locally (optional)\n",
    "print(\" Deployment Package Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(\" Dockerfile - Docker configuration for containerized deployment\")\n",
    "print(\" app.py - Streamlit web application with model inference\")\n",
    "print(\" requirements.txt - Python dependencies for deployment\")\n",
    "print(\" deploy_to_hf.py - Script to push all files to HF Spaces\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n To deploy:\")\n",
    "print(\"1. Run: python tourism_project/deployment/deploy_to_hf.py\")\n",
    "print(\"2. Or use GitHub Actions (automatic on push to main)\")\n",
    "print(\"\\n Files location: tourism_project/deployment/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Created comprehensive Dockerfile with Python 3.9, security configurations, health checks, and Streamlit server setup\n",
    "- Developed interactive Streamlit web application that loads the trained model directly from Hugging Face Model Hub\n",
    "- App provides user-friendly interface to input customer details across 18 features\n",
    "- Implements real-time preprocessing using saved scaler and label encoders for consistent transformations\n",
    "- Returns both binary prediction (Purchase/No Purchase) and probability scores\n",
    "- Created deployment-specific requirements.txt with minimal dependencies for faster container builds\n",
    "- Developed automated deployment script (deploy_to_hf.py) to push all files to Hugging Face Spaces\n",
    "- Deployment uses Docker SDK on HF Spaces for better control and compatibility\n",
    "- All preprocessing artifacts (scalers, encoders) are loaded from the model package ensuring consistency\n",
    "- App includes error handling, progress indicators, and professional UI with metrics and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuCgAW2hktli"
   },
   "source": [
    "# Stage 5: MLOps Pipeline with GitHub Actions\n",
    "\n",
    "## Overview\n",
    "This section sets up end-to-end CI/CD pipeline using GitHub Actions to automate data registration, preparation, model training, and deployment on code updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5BZr5i8PKVN"
   },
   "source": [
    "**Important Setup Note:**\n",
    "\n",
    "Before running the workflow, there's a crucial step - adding the Hugging Face token to GitHub secrets. This allows GitHub Actions to authenticate with Hugging Face when uploading datasets and models.\n",
    "\n",
    "Here's how to do it:\n",
    "1. Go to the GitHub repository settings\n",
    "2. Navigate to Secrets and variables > Actions\n",
    "3. Click \"New repository secret\"\n",
    "4. Name it `HF_TOKEN` and paste the Hugging Face token\n",
    "\n",
    "The YAML file below can be customized based on specific project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Project Requirements File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt for GitHub Actions\n",
    "requirements_content = '''pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "huggingface_hub==0.20.0\n",
    "mlflow==2.9.0\n",
    "xgboost==2.0.3\n",
    "streamlit==1.29.0\n",
    "'''\n",
    "\n",
    "with open(\"tourism_project/requirements.txt\", \"w\") as f:\n",
    "f.write(requirements_content)\n",
    "\n",
    "print(\" requirements.txt created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J029tYPq4Rmq"
   },
   "source": [
    "```\n",
    "name: Tourism Project Pipeline\n",
    "\n",
    "on:\n",
    "push:\n",
    "branches:\n",
    "- main # Automatically triggers on push to the main branch\n",
    "\n",
    "jobs:\n",
    "\n",
    "register-dataset:\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "- name: Install Dependencies\n",
    "run: <add_code_here>\n",
    "- name: Upload Dataset to Hugging Face Hub\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: <add_code_here>\n",
    "\n",
    "data-prep:\n",
    "needs: register-dataset\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "- name: Install Dependencies\n",
    "run: <add_code_here>\n",
    "- name: Run Data Preparation\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: <add_code_here>\n",
    "\n",
    "\n",
    "model-traning:\n",
    "needs: data-prep\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "- name: Install Dependencies\n",
    "run: <add_code_here>\n",
    "- name: Start MLflow Server\n",
    "run: |\n",
    "nohup mlflow ui --host 0.0.0.0 --port 5000 & # Run MLflow UI in the background\n",
    "sleep 5 # Wait for a moment to let the server starts\n",
    "- name: Model Building\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: <add_code_here>\n",
    "\n",
    "\n",
    "deploy-hosting:\n",
    "runs-on: ubuntu-latest\n",
    "needs: [model-traning,data-prep,register-dataset]\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "- name: Install Dependencies\n",
    "run: <add_code_here>\n",
    "- name: Push files to Frontend Hugging Face Space\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: <add_code_here>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9fgZ_Mq3zzp"
   },
   "source": [
    "**Note:** To use this YAML file for our use case, we need to\n",
    "\n",
    "1. Go to the GitHub repository for the project\n",
    "2. Create a folder named ***.github/workflows/***\n",
    "3. In the above folder, create a file named ***pipeline.yml***\n",
    "4. Copy and paste the above content for the YAML file into the ***pipeline.yml*** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create GitHub Actions Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps:**\n",
    "1. Complete the model training section\n",
    "2. Create the Streamlit app\n",
    "3. Set up the `HF_TOKEN` in GitHub repository secrets (Settings > Secrets > Actions)\n",
    "4. Push all files to GitHub repository\n",
    "5. Watch the pipeline automatically trigger on push to main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Actions workflow file created: .github/workflows/pipeline.yml\n",
      "\n",
      "Next steps:\n",
      "1. Add HF_TOKEN to GitHub repository secrets (Settings > Secrets > Actions)\n",
      "2. Push this file to your GitHub repository\n",
      "3. The pipeline will run automatically on push to main branch"
     ]
    }
   ],
   "source": [
    "# Create the complete GitHub Actions workflow YAML file\n",
    "os.makedirs(\".github/workflows\", exist_ok=True)\n",
    "\n",
    "workflow_content = '''name: Tourism Project Pipeline\n",
    "\n",
    "on:\n",
    "push:\n",
    "branches:\n",
    "- main\n",
    "\n",
    "jobs:\n",
    "register-dataset:\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "\n",
    "- name: Set up Python\n",
    "uses: actions/setup-python@v4\n",
    "with:\n",
    "python-version: '3.9'\n",
    "\n",
    "- name: Install Dependencies\n",
    "run: |\n",
    "pip install pandas huggingface_hub\n",
    "\n",
    "- name: Upload Dataset to Hugging Face Hub\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: |\n",
    "python tourism_project/data_registration.py\n",
    "\n",
    "data-prep:\n",
    "needs: register-dataset\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "\n",
    "- name: Set up Python\n",
    "uses: actions/setup-python@v4\n",
    "with:\n",
    "python-version: '3.9'\n",
    "\n",
    "- name: Install Dependencies\n",
    "run: |\n",
    "pip install -r tourism_project/requirements.txt\n",
    "\n",
    "- name: Run Data Preparation\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: |\n",
    "python tourism_project/data_preparation.py\n",
    "\n",
    "model-training:\n",
    "needs: data-prep\n",
    "runs-on: ubuntu-latest\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "\n",
    "- name: Set up Python\n",
    "uses: actions/setup-python@v4\n",
    "with:\n",
    "python-version: '3.9'\n",
    "\n",
    "- name: Install Dependencies\n",
    "run: |\n",
    "pip install -r tourism_project/requirements.txt\n",
    "\n",
    "- name: Start MLflow Server\n",
    "run: |\n",
    "nohup mlflow ui --host 0.0.0.0 --port 5000 &\n",
    "sleep 5\n",
    "\n",
    "- name: Model Building\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: |\n",
    "python tourism_project/model_building/train.py\n",
    "\n",
    "deploy-hosting:\n",
    "runs-on: ubuntu-latest\n",
    "needs: [model-training, data-prep, register-dataset]\n",
    "steps:\n",
    "- uses: actions/checkout@v3\n",
    "\n",
    "- name: Set up Python\n",
    "uses: actions/setup-python@v4\n",
    "with:\n",
    "python-version: '3.9'\n",
    "\n",
    "- name: Install Dependencies\n",
    "run: |\n",
    "pip install huggingface_hub\n",
    "\n",
    "- name: Push files to Hugging Face Space\n",
    "env:\n",
    "HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
    "run: |\n",
    "python tourism_project/deployment/deploy_to_hf.py\n",
    "'''\n",
    "\n",
    "with open(\".github/workflows/pipeline.yml\", \"w\") as f:\n",
    "f.write(workflow_content)\n",
    "\n",
    "print(\" GitHub Actions workflow file created: .github/workflows/pipeline.yml\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Add HF_TOKEN to GitHub repository secrets (Settings > Secrets > Actions)\")\\n",
    "print(\"2. Push this file to your GitHub repository\")\n",
    "print(\"3. The pipeline will run automatically on push to main branch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvEUJ-t5kdxH"
   },
   "source": [
    "## Requirements file for the Github Actions Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Generated Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfqWcLRm-dga"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking MLOps Pipeline Files...\n",
      "======================================================================\n",
      "[PASS] .github/workflows/pipeline.yml\n",
      "[FAIL] tourism_project/requirements.txt\n",
      "[FAIL] tourism_project/data_registration.py\n",
      "[PASS] tourism_project/data_preparation.py\n",
      "[PASS] tourism_project/model_building/train.py\n",
      "[PASS] tourism_project/deployment/app.py\n",
      "[PASS] tourism_project/deployment/Dockerfile\n",
      "[PASS] tourism_project/deployment/requirements.txt\n",
      "[PASS] tourism_project/deployment/deploy_to_hf.py\n",
      "======================================================================\n",
      "\n",
      "Some files are missing. Run previous cells to create them.\n",
      "\n",
      "Summary:\n",
      "- GitHub Actions workflow: Automated CI/CD pipeline\n",
      "- Data scripts: Registration and preparation\n",
      "- Model training: Complete training pipeline with MLflow\n",
      "- Deployment: Streamlit app with Docker configuration"
     ]
    }
   ],
   "source": [
    "# Verify all required files are created\n",
    "print(\"Checking MLOps Pipeline Files...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "files_to_check = [\n",
    "\".github/workflows/pipeline.yml\",\n",
    "\"tourism_project/requirements.txt\",\n",
    "\"tourism_project/data_registration.py\",\n",
    "\"tourism_project/data_preparation.py\",\n",
    "\"tourism_project/model_building/train.py\",\n",
    "\"tourism_project/deployment/app.py\",\n",
    "\"tourism_project/deployment/Dockerfile\",\n",
    "\"tourism_project/deployment/requirements.txt\",\n",
    "\"tourism_project/deployment/deploy_to_hf.py\"\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for file in files_to_check:\n",
    "exists = os.path.exists(file)\n",
    "status = \"[PASS]\" if exists else \"[FAIL]\"\n",
    "print(f\"{status} {file}\")\n",
    "if not exists:\n",
    "all_exist = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "if all_exist:\n",
    "print(\"\\nAll MLOps pipeline files are ready!\")\n",
    "print(\"\\nReady to push to GitHub!\")\n",
    "else:\n",
    "print(\"\\nSome files are missing. Run previous cells to create them.\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(\" - GitHub Actions workflow: Automated CI/CD pipeline\")\n",
    "print(\" - Data scripts: Registration and preparation\")\n",
    "print(\" - Model training: Complete training pipeline with MLflow\")\n",
    "print(\" - Deployment: Streamlit app with Docker configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA6mP-Ebkm3O"
   },
   "source": [
    "## Step 4: GitHub Authentication and Push Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T84Ei-g9Z2uw"
   },
   "source": [
    "* Before moving forward, we need to generate a secret token to push files directly from Colab to the GitHub repository.\n",
    "* Please follow the below instructions to create the GitHub token:\n",
    "- Open your GitHub profile.\n",
    "- Click on ***Settings***.\n",
    "- Go to ***Developer Settings***.\n",
    "- Expand the ***Personal access tokens*** section and select ***Tokens (classic)***.\n",
    "- Click ***Generate new token***, then choose ***Generate new token (classic)***.\n",
    "- Add a note and select all required scopes.\n",
    "- Click ***Generate token***.\n",
    "- Copy the generated token and store it safely in a notepad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPDx4gqGh7cO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git Commands to Run in Terminal:\n",
      "======================================================================\n",
      "\n",
      "# Navigate to your project directory\n",
      "cd \"/Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism Package\"\n",
      "\n",
      "# Initialize Git repository (if not already done)\n",
      "git init\n",
      "\n",
      "# Configure Git (replace with your details)\n",
      "git config user.email \"your-email@example.com\"\n",
      "git config user.name \"Your Name\"\n",
      "\n",
      "# Create GitHub repository first on GitHub.com, then:\n",
      "git remote add origin https://github.com/YOUR_USERNAME/tourism-mlops-project.git\n",
      "\n",
      "# Add all files\n",
      "git add .\n",
      "\n",
      "# Commit changes\n",
      "git commit -m \"Initial commit: Complete MLOps pipeline for tourism package prediction\"\n",
      "\n",
      "# Push to GitHub (will prompt for credentials)\n",
      "git push -u origin main\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Important Notes:\n",
      "1. Create a new repository on GitHub first\n",
      "2. Replace YOUR_USERNAME with your GitHub username\n",
      "3. Use Personal Access Token as password (not your GitHub password)\n",
      "4. Get token from: https://github.com/settings/tokens"
     ]
    }
   ],
   "source": [
    "# Option 1: Using Terminal (Recommended for macOS)\n",
    "# Run these commands in your terminal:\n",
    "\n",
    "commands = '''\n",
    "# Navigate to your project directory\n",
    "cd \"/Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism Package\"\n",
    "\n",
    "# Initialize Git repository (if not already done)\n",
    "git init\n",
    "\n",
    "# Configure Git (replace with your details)\n",
    "git config user.email \"your-email@example.com\"\n",
    "git config user.name \"Your Name\"\n",
    "\n",
    "# Create GitHub repository first on GitHub.com, then:\n",
    "git remote add origin https://github.com/swamu/tourism-mlops-pipeline.git\n",
    "\n",
    "# Add all files\n",
    "git add .\n",
    "\n",
    "# Commit changes\n",
    "git commit -m \"Initial commit: Complete MLOps pipeline for tourism package prediction\"\n",
    "\n",
    "# Push to GitHub (will prompt for credentials)\n",
    "git push -u origin main\n",
    "'''\n",
    "\n",
    "print(\"Git Commands to Run in Terminal:\")\n",
    "print(\"=\"*70)\n",
    "print(commands)\n",
    "print(\"=\"*70)\n",
    "print(\"\\nImportant Notes:\")\n",
    "print(\"1. Create a new repository on GitHub first\")\n",
    "print(\"2. Replace swamu with your GitHub username\")\n",
    "print(\"3. Use Personal Access Token as password (not your GitHub password)\")\n",
    "print(\"4. Get token from: https://github.com/settings/tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuUahCwVigon"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Repository Setup Guide\n",
      "======================================================================\n",
      "\n",
      "Step 1: Create GitHub Repository\n",
      "---------------------------------\n",
      "1. Go to https://github.com/new\n",
      "2. Create a new repository (e.g., 'tourism-mlops-pipeline')\n",
      "3. Do NOT initialize with README, .gitignore, or license\n",
      "\n",
      "Step 2: Add GitHub Secrets\n",
      "--------------------------\n",
      "1. Go to your repository Settings > Secrets and variables > Actions\n",
      "2. Click 'New repository secret'\n",
      "3. Add: HF_TOKEN (your Hugging Face token)\n",
      "\n",
      "Step 3: Push Code\n",
      "-----------------\n",
      "Run these commands in terminal:\n",
      "\n",
      "cd \"/Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism Package\"\n",
      "git init\n",
      "git add .\n",
      "git commit -m \"Complete MLOps pipeline with GitHub Actions\"\n",
      "git branch -M main\n",
      "git remote add origin https://github.com/YOUR_USERNAME/tourism-mlops-pipeline.git\n",
      "git push -u origin main\n",
      "\n",
      "Step 4: Verify Pipeline\n",
      "-----------------------\n",
      "1. Go to your GitHub repo > Actions tab\n",
      "2. You should see the pipeline running automatically\n",
      "3. Monitor each job: register-dataset → data-prep → model-training → deploy-hosting\n",
      "\n",
      "Step 5: Access Deployed App\n",
      "---------------------------\n",
      "Once pipeline completes:\n",
      "- Model: https://huggingface.co/swamu/tourism-prediction-model\n",
      "- App: https://huggingface.co/spaces/swamu/tourism-prediction-app\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Current directory: /Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism Package\n",
      "Git repository not initialized yet"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 2: Programmatic push using GitPython\n",
    "# Install GitPython if needed: pip install gitpython\n",
    "\n",
    "def setup_github_repo():\n",
    "\"\"\"Setup and push to GitHub repository\"\"\"\n",
    "\n",
    "print(\"GitHub Repository Setup Guide\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Instructions\n",
    "steps = \"\"\"\n",
    "Step 1: Create GitHub Repository\n",
    "---------------------------------\n",
    "1. Go to https://github.com/new\n",
    "2. Create a new repository (e.g., 'tourism-mlops-pipeline')\n",
    "3. Do NOT initialize with README, .gitignore, or license\n",
    "\n",
    "Step 2: Add GitHub Secrets\n",
    "--------------------------\n",
    "1. Go to your repository Settings > Secrets and variables > Actions\n",
    "2. Click 'New repository secret'\n",
    "3. Add: HF_TOKEN (your Hugging Face token)\n",
    "\n",
    "Step 3: Push Code\n",
    "-----------------\n",
    "Run these commands in terminal:\n",
    "\n",
    "cd \"/Users/swmukherjee/Documents/www.stage.adobe.com/Wellness Tourism Package\"\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Complete MLOps pipeline with GitHub Actions\"\n",
    "git branch -M main\n",
    "git remote add origin https://github.com/swamu/tourism-mlops-pipeline.git\n",
    "git push -u origin main\n",
    "\n",
    "Step 4: Verify Pipeline\n",
    "-----------------------\n",
    "1. Go to your GitHub repo > Actions tab\n",
    "2. You should see the pipeline running automatically\n",
    "3. Monitor each job: register-dataset → data-prep → model-training → deploy-hosting\n",
    "\n",
    "Step 5: Access Deployed App\n",
    "---------------------------\n",
    "Once pipeline completes:\n",
    "- Model: https://huggingface.co/swamu/tourism-prediction-model\n",
    "- App: https://huggingface.co/spaces/swamu/tourism-prediction-app\n",
    "\"\"\"\n",
    "\n",
    "print(steps)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Current directory info\n",
    "current_dir = Path.cwd()\n",
    "print(f\"\\nCurrent directory: {current_dir}\")\n",
    "\n",
    "# Check if .git exists\n",
    "git_dir = current_dir / \".git\"\n",
    "if git_dir.exists():\n",
    "print(\"Git repository already initialized\")\n",
    "else:\n",
    "print(\"Git repository not initialized yet\")\n",
    "\n",
    "return True\n",
    "\n",
    "setup_github_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-i8Jdyz-_L1"
   },
   "source": [
    "# Stage 6: Output Evaluation and Documentation\n",
    "\n",
    "## Overview\n",
    "This section provides guidelines for documenting the project outcomes, including GitHub repository structure and Hugging Face deployment verification.\n",
    "\n",
    "This section documents the completed MLOps pipeline deployment. Include the following deliverables:\n",
    "\n",
    "1. GitHub Repository with complete codebase and CI/CD pipeline\n",
    "2. Hugging Face Model Hub with trained model\n",
    "3. Hugging Face Spaces with deployed Streamlit application\n",
    "4. Screenshots demonstrating successful execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTK8Bpda_UHg"
   },
   "source": [
    "## Step 1: GitHub Repository Documentation\n",
    "\n",
    "### Required Components\n",
    "\n",
    "1. **Repository Link**\n",
    "- URL: `https://github.com/swamu/tourism-mlops-pipeline`\n",
    "- Public repository with complete project code\n",
    "\n",
    "2. **Folder Structure Screenshot**\n",
    "- Show the complete directory structure\n",
    "- Include all key folders: `.github/workflows/`, `tourism_project/`, etc.\n",
    "- Verify all scripts are present (data_registration.py, data_preparation.py, train.py, etc.)\n",
    "\n",
    "3. **GitHub Actions Workflow Screenshot**\n",
    "- Navigate to: Repository > Actions tab\n",
    "- Show successful pipeline execution\n",
    "- Display all four jobs:\n",
    "* register-dataset (completed)\n",
    "* data-prep (completed)\n",
    "* model-training (completed)\n",
    "* deploy-hosting (completed)\n",
    "- Include execution time and status indicators\n",
    "\n",
    "4. **Code Files Screenshot**\n",
    "- Show key files: pipeline.yml, app.py, Dockerfile\n",
    "- Demonstrate proper code organization\n",
    "\n",
    "### Verification Checklist\n",
    "\n",
    "- [ ] Repository is public and accessible\n",
    "- [ ] All code files are committed\n",
    "- [ ] GitHub Actions workflow exists in `.github/workflows/pipeline.yml`\n",
    "- [ ] HF_TOKEN secret is configured in repository settings\n",
    "- [ ] At least one successful workflow run is visible\n",
    "- [ ] README.md is present with project documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qzzesaG_Xw8"
   },
   "outputs": [],
   "source": [
    "### Expected GitHub Repository Structure\n",
    "\n",
    "```\n",
    "tourism-mlops-pipeline/\n",
    "├── .github/\n",
    "│ └── workflows/\n",
    "│ └── pipeline.yml # CI/CD workflow\n",
    "├── tourism_project/\n",
    "│ ├── data/\n",
    "│ │ ├── tourism.csv # Original dataset\n",
    "│ │ └── processed/\n",
    "│ │ ├── train.csv # Training data\n",
    "│ │ └── test.csv # Testing data\n",
    "│ ├── model_building/\n",
    "│ │ ├── train.py # Training script\n",
    "│ │ └── model_artifacts.pkl # Saved model\n",
    "│ ├── deployment/\n",
    "│ │ ├── app.py # Streamlit app\n",
    "│ │ ├── Dockerfile # Docker config\n",
    "│ │ ├── requirements.txt # Dependencies\n",
    "│ │ └── deploy_to_hf.py # Deployment script\n",
    "│ ├── data_registration.py # Dataset upload script\n",
    "│ ├── data_preparation.py # Data preprocessing\n",
    "│ └── requirements.txt # Project dependencies\n",
    "├── mlruns/ # MLflow tracking (auto-generated)\n",
    "└── README.md # Project documentation\n",
    "```\n",
    "\n",
    "**GitHub Actions Workflow:**\n",
    "- Triggers on push to `main` branch\n",
    "- Jobs run sequentially: register-dataset → data-prep → model-training → deploy-hosting\n",
    "- Each job logs to console for monitoring\n",
    "- Automatic deployment to Hugging Face on success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KDN31V2_YSr"
   },
   "source": [
    "## Step 2: Hugging Face Deployment Documentation\n",
    "\n",
    "### Required Components\n",
    "\n",
    "#### 1. Model Repository\n",
    "\n",
    "**Link**: `https://huggingface.co/swamu/tourism-prediction-model`\n",
    "\n",
    "**Screenshots Required**:\n",
    "- Model repository home page\n",
    "- Files and versions tab showing:\n",
    "* model_artifacts.pkl\n",
    "* README.md with model details\n",
    "- Model card with performance metrics\n",
    "\n",
    "#### 2. Streamlit Application on Hugging Face Spaces\n",
    "\n",
    "**Link**: `https://huggingface.co/spaces/swamu/tourism-prediction-app`\n",
    "\n",
    "**Screenshots Required**:\n",
    "- Space home page showing app status (Running)\n",
    "- Live Streamlit application interface showing:\n",
    "* Application title and description\n",
    "* Input form with all 18 customer feature fields\n",
    "* Example prediction results\n",
    "* Purchase probability display\n",
    "* Recommendation output\n",
    "- Space files tab showing:\n",
    "* app.py\n",
    "* Dockerfile\n",
    "* requirements.txt\n",
    "* README.md\n",
    "\n",
    "#### 3. Application Functionality\n",
    "\n",
    "**Demonstration Screenshots**:\n",
    "1. **Initial State**: Empty form ready for input\n",
    "2. **Filled Form**: All fields populated with sample customer data\n",
    "3. **Prediction Output**:\n",
    "- Purchase probability percentage\n",
    "- Prediction class (Likely to Purchase / Unlikely to Purchase)\n",
    "- Recommendation text\n",
    "4. **Model Loading**: Success message showing model loaded from HF Hub\n",
    "\n",
    "### Verification Checklist\n",
    "\n",
    "- [ ] Model repository is public and accessible\n",
    "- [ ] Model artifacts file is uploaded\n",
    "- [ ] Streamlit Space is deployed and running\n",
    "- [ ] Docker container built successfully\n",
    "- [ ] Application loads without errors\n",
    "- [ ] Predictions work correctly with sample data\n",
    "- [ ] All 18 input fields are functional\n",
    "- [ ] Results display properly formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuIUdj3b_ZYV"
   },
   "source": [
    "### Expected Hugging Face Spaces App\n",
    "\n",
    "#### App Features\n",
    "- Modern, responsive UI with Streamlit\n",
    "- 18 input fields for customer features\n",
    "- Real-time predictions with probability scores\n",
    "- Visual indicators for purchase likelihood\n",
    "- Actionable recommendations for sales team\n",
    "\n",
    "#### Deployment Details\n",
    "- Hosted on Hugging Face Spaces with Docker\n",
    "- Automatically loads model from HF Model Hub\n",
    "- Scalable and always available\n",
    "- Updates automatically when model is retrained\n",
    "\n",
    "#### Access URLs\n",
    "- Model Repository: `https://huggingface.co/swamu/tourism-prediction-model`\n",
    "- Live App: `https://huggingface.co/spaces/swamu/tourism-prediction-app`\n",
    "\n",
    "#### Usage Instructions\n",
    "1. Open the Streamlit app URL\n",
    "2. Fill in customer information\n",
    "3. Click \"Predict Purchase Probability\"\n",
    "4. Get instant prediction with confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Project Documentation\n",
    "\n",
    "### Complete MLOps Pipeline\n",
    "\n",
    "This project implements an end-to-end MLOps pipeline with:\n",
    "\n",
    "#### 1. Data Management\n",
    "- Dataset registration on Hugging Face Hub\n",
    "- Automated data preprocessing and splitting\n",
    "- Version-controlled datasets\n",
    "\n",
    "#### 2. Model Development\n",
    "- 6 ML algorithms trained and compared (Decision Tree, Random Forest, Bagging, AdaBoost, Gradient Boosting, XGBoost)\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- MLflow experiment tracking\n",
    "- Best model: Gradient Boosting with F1 Score: 0.8522\n",
    "\n",
    "#### 3. Deployment\n",
    "- Interactive Streamlit web application\n",
    "- Dockerized deployment\n",
    "- Hosted on Hugging Face Spaces\n",
    "\n",
    "#### 4. CI/CD Pipeline\n",
    "- GitHub Actions workflow\n",
    "- Automated testing and deployment\n",
    "- Continuous integration on code updates\n",
    "\n",
    "#### 5. Monitoring and Tracking\n",
    "- MLflow for experiment tracking\n",
    "- Model versioning on Hugging Face\n",
    "- Performance metrics logged\n",
    "\n",
    "### Key Technologies\n",
    "- ML and Data: scikit-learn, XGBoost, pandas\n",
    "- Tracking: MLflow\n",
    "- Deployment: Streamlit, Docker, Hugging Face\n",
    "- CI/CD: GitHub Actions\n",
    "- Version Control: Git, Hugging Face Hub\n",
    "\n",
    "### Next Steps\n",
    "1. Run all notebook cells to generate files\n",
    "2. Push code to GitHub\n",
    "3. Configure HF_TOKEN in GitHub Secrets (needed for authentication)\n",
    "4. Monitor GitHub Actions pipeline\n",
    "5. Access deployed app on Hugging Face Spaces\n",
    "\n",
    "### Conclusion\n",
    "MLOps Pipeline Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md created successfully!\n",
      "Location: README.md\n",
      "\n",
      "This README provides:\n",
      "- Project overview and architecture\n",
      "- Model performance metrics\n",
      "- Setup and installation instructions\n",
      "- CI/CD pipeline documentation\n",
      "- Links to deployed resources"
     ]
    }
   ],
   "source": [
    "# Create README.md for GitHub repository\n",
    "readme_content = '''# Tourism Package Prediction MLOps Pipeline\n",
    "\n",
    "[![GitHub Actions](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-blue)](https://github.com/features/actions)\n",
    "[![Hugging Face](https://img.shields.io/badge/-Hugging%20Face-yellow)](https://huggingface.co/)\n",
    "[![MLflow](https://img.shields.io/badge/MLflow-Tracking-orange)](https://mlflow.org/)\n",
    "[![Streamlit](https://img.shields.io/badge/Streamlit-App-red)](https://streamlit.io/)\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "An end-to-end MLOps pipeline for predicting customer purchase behavior for the Wellness Tourism Package. This project implements automated data processing, model training with experiment tracking, and deployment using CI/CD best practices.\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "\"Visit with Us,\" a leading travel company, needs to efficiently identify customers likely to purchase the Wellness Tourism Package. This ML solution automates customer targeting, improving marketing efficiency and conversion rates.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Data Registration → Data Preparation → Model Training → Model Deployment\n",
    "↓ ↓ ↓ ↓\n",
    "Hugging Face Train/Test Split MLflow Tracking Streamlit App\n",
    "Dataset Hub Feature Engineering Model Selection Docker Container\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Automated Data Pipeline**: Registers and preprocesses datasets on Hugging Face Hub\n",
    "- **ML Experimentation**: Trains 6 algorithms with hyperparameter tuning and MLflow tracking\n",
    "- **Best Model**: Gradient Boosting with F1 Score of 0.8522\n",
    "- **CI/CD**: GitHub Actions workflow for automated testing and deployment\n",
    "- **Interactive UI**: Streamlit web app for real-time predictions\n",
    "- **Containerized**: Docker-based deployment on Hugging Face Spaces\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "| Model | F1 Score | Accuracy | Precision | Recall | ROC AUC |\n",
    "|-------|----------|----------|-----------|--------|---------|\n",
    "| **Gradient Boosting** | **0.8522** | **0.9479** | **0.9394** | **0.7799** | **0.9787** |\n",
    "| Bagging | 0.8293 | 0.9407 | 0.9297 | 0.7484 | 0.9835 |\n",
    "| Random Forest | 0.7636 | 0.9213 | 0.9052 | 0.6604 | 0.9752 |\n",
    "| Decision Tree | 0.7103 | 0.8983 | 0.7863 | 0.6478 | 0.8116 |\n",
    "| AdaBoost | 0.3850 | 0.8414 | 0.7593 | 0.2579 | 0.8272 |\n",
    "\n",
    "## Technologies\n",
    "\n",
    "- **ML/Data**: Python, pandas, scikit-learn, XGBoost\n",
    "- **Experiment Tracking**: MLflow\n",
    "- **Deployment**: Streamlit, Docker, Hugging Face Spaces\n",
    "- **CI/CD**: GitHub Actions\n",
    "- **Version Control**: Git, Hugging Face Hub\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "tourism-mlops-pipeline/\n",
    "├── .github/workflows/\n",
    "│ └── pipeline.yml # CI/CD workflow\n",
    "├── tourism_project/\n",
    "│ ├── data/ # Dataset files\n",
    "│ ├── model_building/ # Training scripts\n",
    "│ ├── deployment/ # Deployment files\n",
    "│ ├── data_registration.py\n",
    "│ ├── data_preparation.py\n",
    "│ └── requirements.txt\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.9+\n",
    "- Hugging Face account and token\n",
    "- GitHub account\n",
    "\n",
    "### Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/swamu/tourism-mlops-pipeline.git\n",
    "cd tourism-mlops-pipeline\n",
    "```\n",
    "\n",
    "2. Install dependencies:\n",
    "```bash\n",
    "pip install -r tourism_project/requirements.txt\n",
    "```\n",
    "\n",
    "3. Set environment variables:\n",
    "```bash\n",
    "export HF_TOKEN=your_huggingface_token\n",
    "```\n",
    "\n",
    "### Running Locally\n",
    "\n",
    "1. **Data Preparation**:\n",
    "```bash\n",
    "python tourism_project/data_preparation.py\n",
    "```\n",
    "\n",
    "2. **Model Training**:\n",
    "```bash\n",
    "python tourism_project/model_building/train.py\n",
    "```\n",
    "\n",
    "3. **Run Streamlit App**:\n",
    "```bash\n",
    "cd tourism_project/deployment\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "## CI/CD Pipeline\n",
    "\n",
    "The GitHub Actions workflow automatically:\n",
    "1. Registers dataset on Hugging Face\n",
    "2. Prepares and splits data\n",
    "3. Trains models with MLflow tracking\n",
    "4. Deploys the best model to Hugging Face Spaces\n",
    "\n",
    "### Setup GitHub Actions\n",
    "\n",
    "Getting the automated pipeline running is straightforward:\n",
    "\n",
    "1. **Configure the token**: Head to repository Settings > Secrets and variables > Actions, then add a new secret named `HF_TOKEN` with the Hugging Face token value\n",
    "2. **Push the code**: Once pushed to the main branch, the workflow kicks off automatically\n",
    "3. **Watch it run**: The Actions tab shows real-time progress of each pipeline stage\n",
    "\n",
    "## Deployment\n",
    "\n",
    "- **Model**: [huggingface.co/swamu/tourism-prediction-model](https://huggingface.co/swamu/tourism-prediction-model)\n",
    "- **Live App**: [huggingface.co/spaces/swamu/tourism-prediction-app](https://huggingface.co/spaces/swamu/tourism-prediction-app)\n",
    "\n",
    "## MLflow Tracking\n",
    "\n",
    "View experiment runs and metrics:\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "Navigate to `http://localhost:5000`\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Contributions are welcome! Please open an issue or submit a pull request.\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License.\n",
    "\n",
    "## Authors\n",
    "\n",
    "- Your Name - MLOps Engineer\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- \"Visit with Us\" travel company for the business case\n",
    "- Hugging Face for hosting infrastructure\n",
    "- MLflow for experiment tracking\n",
    "\n",
    "---\n",
    "\n",
    "Star this repository if you find it helpful!\n",
    "'''\n",
    "\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "f.write(readme_content)\n",
    "\n",
    "print(\"README.md created successfully!\")\n",
    "print(\"Location: README.md\")\n",
    "print(\"\\nThis README provides:\")\n",
    "print(\" - Project overview and architecture\")\n",
    "print(\" - Model performance metrics\")\n",
    "print(\" - Setup and installation instructions\")\n",
    "print(\" - CI/CD pipeline documentation\")\n",
    "print(\" - Links to deployed resources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evidence Documentation Template\n",
    "\n",
    "### GitHub Repository Evidence\n",
    "\n",
    "```\n",
    "Repository URL: https://github.com/swamu/tourism-mlops-pipeline\n",
    "\n",
    "Folder Structure:\n",
    "- Project contains all required directories\n",
    "- GitHub Actions workflow configured\n",
    "- All Python scripts present and functional\n",
    "\n",
    "Workflow Execution:\n",
    "- Pipeline triggered on: [DATE]\n",
    "- All jobs completed successfully\n",
    "- Execution time: [TIME]\n",
    "- Status: SUCCESS\n",
    "```\n",
    "\n",
    "### Hugging Face Evidence\n",
    "\n",
    "```\n",
    "Model Repository: https://huggingface.co/swamu/tourism-prediction-model\n",
    "- Model Size: [SIZE] MB\n",
    "- Last Updated: [DATE]\n",
    "- Files: model_artifacts.pkl, README.md\n",
    "\n",
    "Spaces Application: https://huggingface.co/spaces/swamu/tourism-prediction-app\n",
    "- Status: Running\n",
    "- SDK: Docker\n",
    "- Last Build: [DATE]\n",
    "- Application accessible and functional\n",
    "```\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "```\n",
    "Best Model: Gradient Boosting\n",
    "- F1 Score: 0.8522\n",
    "- Accuracy: 0.9479\n",
    "- Precision: 0.9394\n",
    "- Recall: 0.7799\n",
    "- ROC AUC: 0.9787\n",
    "\n",
    "Training Dataset: 3,302 samples\n",
    "Testing Dataset: 826 samples\n",
    "Total Features: 18\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "Provide the following in your submission:\n",
    "1. Links to all deployed resources\n",
    "2. Screenshots demonstrating successful deployment\n",
    "3. Evidence of successful pipeline execution\n",
    "4. Sample predictions from the deployed application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MLOps PIPELINE PROJECT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Project: Tourism Package Prediction MLOps Pipeline\n",
      "Objective: Predict customer purchase behavior using ML with full CI/CD\n",
      "\n",
      "Components Implemented:\n",
      "1. Data Management: Registration and preprocessing on Hugging Face\n",
      "2. Model Training: 6 algorithms with MLflow tracking\n",
      "3. Deployment: Dockerized Streamlit app on HF Spaces\n",
      "4. CI/CD: GitHub Actions automated pipeline\n",
      "5. Monitoring: MLflow experiment tracking and versioning\n",
      "\n",
      "Best Model Performance:\n",
      "- Algorithm: Gradient Boosting\n",
      "- F1 Score: 0.8522\n",
      "- Accuracy: 0.9479\n",
      "- ROC AUC: 0.9787\n",
      "\n",
      "Deployment URLs:\n",
      "- GitHub: https://github.com/YOUR_USERNAME/tourism-mlops-pipeline\n",
      "- Model: https://huggingface.co/YOUR_USERNAME/tourism-prediction-model\n",
      "- App: https://huggingface.co/spaces/YOUR_USERNAME/tourism-prediction-app\n",
      "\n",
      "======================================================================\n",
      "PROJECT COMPLETE\n",
      "======================================================================"
     ]
    }
   ],
   "source": [
    "# Generate final project summary\n",
    "print(\"=\"*70)\n",
    "print(\"MLOps PIPELINE PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nProject: Tourism Package Prediction MLOps Pipeline\")\n",
    "print(\"Objective: Predict customer purchase behavior using ML with full CI/CD\")\n",
    "print(\"\\nComponents Implemented:\")\n",
    "print(\"1. Data Management: Registration and preprocessing on Hugging Face\")\n",
    "print(\"2. Model Training: 6 algorithms with MLflow tracking\")\n",
    "print(\"3. Deployment: Dockerized Streamlit app on HF Spaces\")\n",
    "print(\"4. CI/CD: GitHub Actions automated pipeline\")\n",
    "print(\"5. Monitoring: MLflow experiment tracking and versioning\")\n",
    "print(\"\\nBest Model Performance:\")\n",
    "print(\"- Algorithm: Gradient Boosting\")\n",
    "print(\"- F1 Score: 0.8522\")\n",
    "print(\"- Accuracy: 0.9479\")\n",
    "print(\"- ROC AUC: 0.9787\")\n",
    "print(\"\\nDeployment URLs:\")\n",
    "print(\"- GitHub: https://github.com/swamu/tourism-mlops-pipeline\")\n",
    "print(\"- Model: https://huggingface.co/swamu/tourism-prediction-model\")\n",
    "print(\"- App: https://huggingface.co/spaces/swamu/tourism-prediction-app\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "klg2JF-oBblG",
    "m0CcOjZ-BblL",
    "zm6bNQOJBblO",
    "z8C11AzTBblP",
    "0LbSu_p2jYfe",
    "9DtS3gNDjBbR",
    "hh2TjRG5WJ4Z",
    "eZZKnLkLjeM4",
    "0McYCZzkji5I",
    "9QrY2v77vbEZ",
    "LCvrklrBwNvJ",
    "07cYzWcIwTL-",
    "V4ynzpKNwWS_",
    "PuCgAW2hktli",
    "PvEUJ-t5kdxH",
    "BA6mP-Ebkm3O",
    "v-i8Jdyz-_L1"
   ],
   "provenance": []
  },
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}